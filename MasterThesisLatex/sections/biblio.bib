% 1 - Introduction
%%%%%%%%%%%%

% http://boritcag.org/pdf/Asbestos%20Detection%20Techniques%20for%20Air%20and%20Soil%20August%202004%20(Anthony%20Perry).pdf

% https://en.wikipedia.org/wiki/Asbestosis
% https://en.wikipedia.org/wiki/Mesothelioma

@misc{asbestosMaacenter,
  title = {{Asbestos - Asbestos is a mineral that has been utilized in thousands of products, but exposure to the toxin can cause mesothelioma and other diseases}},
  howpublished = "\url{https://www.maacenter.org/asbestos/}",
  year = {2019}, 
  note = "[Online; accessed 12-March-2019]"
}

@misc{asbestosisWiki,
  title = {{Asbestosis}},
  howpublished = "\url{https://en.wikipedia.org/wiki/Asbestosis}",
  year = {2019}, 
  note = "[Online; accessed 12-March-2019]"
}

@misc{MesotheliomaWiki,
  title = {{Mesothelioma}},
  howpublished = "\url{https://en.wikipedia.org/wiki/Mesothelioma}",
  year = {2019}, 
  note = "[Online; accessed 12-March-2019]"
}

@misc{environmental2008framework,
  title={Framework for Investigating Asbestos-contaminated Superfund Sites},
  author={Environmental Protection Agency (EPA)},
  year={2008},
  publisher={OSWER Directive 9200.0-68}
}

% 2 - Analysis
%%%%%%%%%%%%

% 2.3 DeepLearning Frameworks

@misc{tensorflow,
  title = {{Tensorflow - An end-to-end open source machine learning platform}},
  howpublished = "\url{https://www.tensorflow.org/}",
  year = {2019}, 
  note = "[Online; accessed 11-March-2019]"
}

@misc{pytorch,
  title = {{Pytorch - From research to production}},
  howpublished = "\url{https://pytorch.org/}",
  year = {2019}, 
  note = "[Online; accessed 11-March-2019]"
}

@misc{pytorchOnePointZero,
  title = {{Announcing PyTorch 1.0 for both research and production}},
  howpublished = "\url{https://developers.facebook.com/blog/post/2018/05/02/announcing-pytorch-1.0-for-research-production/}",
  year = {2019}, 
  note = "[Online; accessed 11-March-2019]"
}

@misc{keras,
  title = {{Keras: The Python Deep Learning library}},
  howpublished = "\url{https://keras.io/}",
  year = {2019}, 
  note = "[Online; accessed 11-March-2019]"
}

@misc{deepdiva,
  title = {{DeepDIVA - A Highly-Functional Python Framework for Reproducible Experiments}},
  howpublished = "\url{https://diva-dia.github.io/DeepDIVAweb/}",
  year = {2019}, 
  note = "[Online; accessed 11-March-2019]"
}

% 2.2 Related Work

@article{perry2004discussion,
  title={A discussion of asbestos detection techniques for air and soil},
  author={Perry, Anthony and others},
  journal={Washington, US EPA},
  year={2004}
}


% 2.4 Tools

@misc{viztoolbox,
  title = {{Convolutional Neural Network Filter Visualization}},
  howpublished = "\url{https://github.com/utkuozbulak/pytorch-cnn-visualizations#convolutional-neural-network-filter-visualization/}",
  year = {2019}, 
  note = "[Online; accessed 11-March-2019]"
}




% 4 - Design and Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4.x - Data Augmentation

% VGG
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

% ResNet
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

% 4.x - Transfer Learning

@inproceedings{sharif2014cnn,
  title={CNN features off-the-shelf: an astounding baseline for recognition},
  author={Sharif Razavian, Ali and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={806--813},
  year={2014}
}

@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}

@article{sermanet2013overfeat,
  title={Overfeat: Integrated recognition, localization and detection using convolutional networks},
  author={Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Micha{\"e}l and Fergus, Rob and LeCun, Yann},
  journal={arXiv preprint arXiv:1312.6229},
  year={2013}
}

@inproceedings{carreira2016human,
  title={Human pose estimation with iterative error feedback},
  author={Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4733--4742},
  year={2016}
}

@inproceedings{dai2016instance,
  title={Instance-aware semantic segmentation via multi-task network cascades},
  author={Dai, Jifeng and He, Kaiming and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3150--3158},
  year={2016}
}

@inproceedings{donahue2015long,
  title={Long-term recurrent convolutional networks for visual recognition and description},
  author={Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2625--2634},
  year={2015}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3128--3137},
  year={2015}
}

% Experiments show that ImageNet pre-training speeds up convergence early in training,
% but not necessarily provide regularization or improve final target task accuracies.
@article{he2018rethinking,
  title={Rethinking ImageNet pre-training},
  author={He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  journal={arXiv preprint arXiv:1811.08883},
  year={2018}
}

% Huh et al. question if pre-training on ImageNet really is that data-hungry since
% reducing classes or images in the pre-training resulted in very modest accuracy
% decline in the target domain !!! They basically play around with AlexNet pre-training
% to find the important parameters why pre-training is good and when it starts to
% deteriorate.
@article{huh2016makes,
  title={What makes ImageNet good for transfer learning?},
  author={Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A},
  journal={arXiv preprint arXiv:1608.08614},
  year={2016}
}

% SIGOPT



% 5 Evaluation
%%%%%%%%%%%%%%