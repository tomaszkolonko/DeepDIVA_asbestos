\chapter{Conclusion}

\section{Summary and Conclusion}

Contrary to some believes that transfer learning is not necessarily a good option if using across domains, this work shows that even for microscopic images with one object being looked for, pre-training on ImageNet is a good and valid option. Overall, transfer learning led to improved accuracies by roughly 8\% and often yielded faster convergence. Especially in the asbestos task, where only about 1'000 images (around 500 per class) were used for training, transfer learning boosted accuracy. It was surprising though, that even when the models were run for 100 or 200 epochs from scratch, they did usually not catch up with the pre-trained models. Visualizations of the layer activations have shown that when the model is trained from scratch no complex (mid-level and high-level) patterns are learned. That can be mostly attributed to the small size of the dataset and the less than ideal quality of the images. Pre-training provided some general purpose filters that were applicable to the asbestos task, thus leading to better accuracy.

Visualizing the layers and creating heatmaps was much more difficult than expected. Finding libraries was not straight forward and adding them to PyTorch was a struggle. Every architecture has its own model representation which would need some transformations into another format, known by the library. CNN-layer visualizations were quite insightful and allowed better understanding of the architecture as well as the shortcomings of the dataset. The heatmap visualizations were less easy to interpret since different layers showed partially contradicting areas of interest for a certain class.

Cropping was especially difficult since the asbestos fibers were not spaced evenly over the image. So cropping would always lead to some crops with and some crops without asbestos. Feeding all the crops to the network and averaging its output before updating the weights, can only lead to better performance when more crops include asbestos than not. Cropping still remains an important and valid method since it does reduce overfitting and can be used for online data augmentation. The problem of retaining the correct labels while cropping needs to be solved in order for cropping to work better.

Since the quality and size of the dataset is clearly a problem, several variations of it were tested in order to achieve a better data-subset. One variation of the dataset removed all very unclear or faulty images from training. Another variation reduced it even more to contain only clear and good examples of asbestos fibers spanning more than half of the image. The intention was to give the network high-quality sample images to learn from, while online data augmentation could compensate for the fewer images. Yet another variation was to extend the training set by adding all validation images to the training. Although sometimes slightly better performance was achieved, these alterations to the dataset failed to provide a means to boost accuracy.

Modifications to the network were like a drastic reduction of the learnable parameters by reducing filters and units in the fully connected layers lead to the same accuracies or even increased accuracies by a small amount. More interestingly was that a reduction of the complexity of the network by roughly 99\% had no detrimental effect.

It is safe to say that the quality of the current dataset is the limiting factor since all the architectures reach values between 80\% and 90\% and never cross the 90\% threshold regardless of different cropping and data augmentation methods, different dataset considerations and different modifications to the architectures. The accuracies stay in the same range although parameter reductions of 99.99\% were achieved. The biggest advantages in reducing the parameter size is faster learning, better interpretibility and better deployment of the model to embedded systems.

\section{Future Work}

As mentioned in several papers, the pre-processing of the image itself might play a vital role in the ease of asbestos detection. For example, thresholding and binarization reduce the noise in the image and transform a grayscale image into a black and white image. This allows to clearly identify the asbestos-like structures and could potentially lead to architectures with fewer parameters and better performance. Although Deep Learning architectures should extract the needed features by themselves, as seen with the visualizations that are not necessarily the case with only a few images. In that specific case, pre-processing of the images could help.\\

The dataset quality is of utmost importance. Future work could be channeled into producing a much bigger high-quality dataset with good images and correct labels.\\

Continued work on the visualizations could be very rewarding. Especially making the visual toolbox work for other architectures and to visualize models that were trained only on grayscale colors instead of RGB. Training on grayscale colors (having only one channel in the input volume) resulted in the same accuracies as training with RGB. Visualizations failed on these architectures taking only one channel instead of three but better visualizations could potentially be expected from this change.

Reducing the number of parameters by over 99\% did not harm the performance and even improved in some cases. Transfer learning cannot be used in such a scenario since most of the feature mappings would be reduced as well. Being able to pinpoint the filters that lead to the highest activations and transfer only them to the remaining filters in the reduced architectures, could lead to better performance and less complexity.\\
