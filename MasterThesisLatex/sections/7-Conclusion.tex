\chapter{Conclusion}

\section{Summary and Conclusion}

Contrary to some believes that transfer learning is not necessarily a good option if using across domains, this work shows that even for microscopic images with one object being looked for, pre-training on ImageNet is a good option. Overall, transfer learning led to improved accuracies by roughly 8\% and often yielded faster convergence. Especially in the asbestos task, where only about 1'000 (around 500 per class) images were used for training, transfer learning helped a lot. It was surprising though, that even when the models were run 3 times longer from scratch, they did usually not catch up with the pre-trained models. Visualizations of the layer activations have shown that when the model is trained from scratch no complex (mid-level and high-level) patterns are learned. I argue that is due to the less than ideal quality and size of the dataset. Pre-training provided some general purpose filters that were applicable to the asbestos task, thus leading to better accuracy.

Visualizing the layers and creating heatmaps was much more difficult than expected. Finding libraries was difficult and implementing them in PyTorch was a struggle. Every architecture has its own model representation which would need transforming it into another format, known by the library. But the visualization on VGG13 was not as insightful as initially hoped for. Maybe it was due to the network simply not learning any better features. Heatmaps were also difficult to interpret since different layers showed partially contradicting areas of interest for a certain class.

Cropping was especially difficult since the asbestos fibers were not spaced evenly over the image. So cropping would always lead to some crops with and some crops without asbestos. Feeding all the crops to the network and averaging its output before updating the weights, can only lead to better performance when more crops include asbestos than not. Still, cropping with somehow retaining the correct label is still the way to go since variation and randomness are introduced and the small size of the dataset is efficiently mitigated.

Since the quality and size of the dataset is clearly a problem, I tried to change it in certain ways in order to achieve higher quality. One variation of the dataset removed all very unclear or faulty images from training. Another time I reduced it even more to contain only clear and good examples. My intention was to give the network high-quality example images to learn from correctly and then apply the knowledge on all the images, unclear images included. Another time I tried to extend the training set by adding all validation images to the training. Although sometimes slightly better performance was achieved, it wasn't really significant.

Modifications to the network were like a drastic reduction of the learnable parameters by reducing filters and units in the fully connected layers lead to the same accuracies or even increased accuracies by a small amount. More interestingly was that a reduction of the complexity of the network by roughly 99\% had no detrimental effect.

All in all, I think that the quality of the current dataset is the limiting factor since all the architectures reach values between 80\% and 90\% and never cross the 90\% threshold regardless of different cropping and data augmentation methods, different dataset considerations and different modifications to the architectures. The accuracies stay more or less the same although I was able to achieve parameter reductions of 99.99\% which makes the asbestos recognition task easily deployable and efficient.

\section{Future Work}

As mentioned in several papers, the pre-processing of the image itself might play a vital role in the ease of asbestos detection. For example, thresholding and binarization reduce the noise in the image and transform a grayscale image into a black and white image. This allows to clearly identify the asbestos-like structures and could potentially lead to architectures with fewer parameters and better performance. Although Deep Learning architectures should extract the needed features by themselves, as seen with the visualizations that are not necessarily the case with only a few images. In that specific case, pre-processing of the images could help.\\

The dataset quality is of utmost importance. Future work could be channeled into producing a much bigger high-quality dataset with good images and correct labels.\\

Continued work on the visualizations could be very rewarding. Especially making the visual toolbox work for other architectures and to visualize models that were trained only on grayscale colors instead of RGB. Training on grayscale colors (having only one channel in the input volume) resulted in the same accuracies as training with RGB. I would expect to have more interesting visualizations and less noise when focusing on grayscale values.\\

Reducing the number of parameters by over 99\% did not harm the performance and even improved in some cases. Transfer learning cannot be used in such a scenario since most of the feature mappings would be reduced as well. Being able to pinpoint the filters that lead to the highest activations and transfer only them to the remaining filters in the reduced architectures, could lead to better performance and less complexity.\\
