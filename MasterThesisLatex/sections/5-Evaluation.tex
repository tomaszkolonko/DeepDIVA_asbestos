\chapter{Evaluation}

As mentioned in Chapter 4 there is no reliable baseline and one needs to be established first with some basic CNN architectures. Then different architectures will be compared against each other with and without pre-training in order to find out if transfer learning is a valid option for this problem task. Data augmentation should add regularization and lead to a model that is better generalizable to the test set. New architectures will then be created to fit better the problem at hand.

\section{Baseline}

\subsection{CNN\_Basic}

\subsection{AlexNet}

For the baseline with AlexNet the tests were 5 tim

For the baseline two simple CNN architectures are used. A three layered basic CNN and AlexNet. Both are optimized with gridsearch for the hyper parameter learning rate while the Adam optimizer was used. They were both run for 50 epochs with a learning rate decay of 15. A learning rate decay is defined as reducing the learning rate by a factor of 10 every N epochs:

\[ lr = lr * (0.1^{\frac{epoch}{decay}}) \]


Every run as shown in table \ref{tbl:similarity-test-map} and \ref{tbl:similarity-test-map} has been run for 5 times and averaged across all runs in order to obtain robust values.


\begin{table*}[t]
	\ra{1.3}
    \caption{Accuracy (\%) for several learning rates and lr-decays for AlexNet as a baseline.}
    \centering
    \begin{small}
	\textsc{
      \resizebox{0.99\textwidth}{!}{%
      \begin{tabular}{rcclcclcc}
      \toprule 
      & \multicolumn{2}{c}{Learning-Rate decay: 10} && %
        \multicolumn{2}{c}{Learning-Rate decay: 15} && %
        \multicolumn{2}{c}{Learning-Rate decay: 20} \\
      \cmidrule{2-3} \cmidrule{5-6}  \cmidrule{8-9}
      & learning rate & accuracy  && %
        learning rate & accuracy  && %
        learning rate & accuracy  \\ 
      \midrule
      AlexNet		& 0.1 & 59.80\%  &&  0.1 & 10\% &&  0.1 & 10\% \\
      AlexNet		& 0.05 & 59.80\%  &&  0.05 & 10\% && 0.05 & 10\% \\
      AlexNet		& 0.01 & 59.80\%  &&  0.01 & 10\% &&  0.01 & 10\% \\
      AlexNet		& 0.005 & 59.80\%  &&  0.005 & 10\% &&  0.005 & 10\% \\
      AlexNet		& 0.001 & 74.75\%  &&  0.001 & 10\% &&  0.001 & 10\% \\
      AlexNet		& 0.0005 & 74.75\%  &&  0.0005 & 10\% &&  0.0005 & 10\% \\
      AlexNet		& 0.0001 & 74.75\%  &&  0.0001 & 10\% &&  0.0001 & 10\% \\
    \bottomrule
    \end{tabular}}
	}
    \end{small}
    %\end{center}
    \vspace{-3.9mm}
    \label{tbl:alexnet-baseline}
\end{table*}

adfasdfasdfasdfasdfasdf
asdfasdfasdf

\begin{table*}[t]
	\ra{1.3}
    \caption{Accuracy (\%) for several learning rates and lr-decays for CNN\_Basic as a baseline.}
    \centering
    \begin{small}
	\textsc{
      \resizebox{0.99\textwidth}{!}{%
      \begin{tabular}{rcclcclcc}
      \toprule 
      & \multicolumn{2}{c}{Learning-Rate decay: 10} && %
        \multicolumn{2}{c}{Learning-Rate decay: 15} && %
        \multicolumn{2}{c}{Learning-Rate decay: 20} \\
      \cmidrule{2-3} \cmidrule{5-6}  \cmidrule{8-9}
      & learning rate & accuracy  && %
        learning rate & accuracy  && %
        learning rate & accuracy  \\ 
      \midrule
      CNN\_BASIC		& 0.1 & 59.80\%  &&  0.1 & 10\% &&  0.1 & 10\% \\
      CNN\_BASIC		& 0.05 & 59.80\%  &&  0.05 & 10\% && 0.05 & 10\% \\
      CNN\_BASIC		& 0.01 & 59.80\%  &&  0.01 & 10\% &&  0.01 & 10\% \\
      CNN\_BASIC		& 0.005 & 59.80\%  &&  0.005 & 10\% &&  0.005 & 10\% \\
      CNN\_BASIC		& 0.001 & 74.75\%  &&  0.001 & 10\% &&  0.001 & 10\% \\
      CNN\_BASIC		& 0.0005 & 74.75\%  &&  0.0005 & 10\% &&  0.0005 & 10\% \\
      CNN\_BASIC		& 0.0001 & 74.75\%  &&  0.0001 & 10\% &&  0.0001 & 10\% \\
    \bottomrule
    \end{tabular}}
	}
    \end{small}
    %\end{center}
    \vspace{-3.9mm}
    \label{tbl:cnn-basic-baseline}
\end{table*}


\section{ResNet18}

\section{Densenet}

\section{Inception}


