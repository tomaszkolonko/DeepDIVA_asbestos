\chapter{Deep Learning}

Deep Learning (DL) is a subfield of Neural Networks (NN) which in turn is a subfield of Machine Learning (ML) which again is a subfield of Artificial Intelligence (AI). Figure \ref{fig:MLOverview} shows a Neural Network relevant taxonomy of AI. There are many advantages of ML over more traditional statistical methods as often found in statistical learning. Some of them are that ML does not require any hypothesis and deep understanding of the underlying data. While statistical learning is mostly about inference (deducing properties of an underlying probability distribution, sampled from a larger population), ML is mostly about predictions in supervised, unsupervised and semi-supervised learning. It also does not operate on assumptions like normality, multicollinearity and homoscedasticity etc.. Statistical Learning operates on much smaller datasets with only a few attributes and therefore is less fit for problems with millions of possible and possibly unknown attributes and data samples. Machine Learning excels at identifying patterns from large datsets through iterations and being able to predict or classify previously unseen data.
Traditional Machine Learning approaches were based on very specific feature extractions that needed to be found/created manually and could take months to fine tune. The biggest downside was that the features often were not generalizable but were very domain specific. Even looking at the same object from different angles or from different photographs often needed additional fine tuning of existing features or creating new ones. Some of the feature extraction algorithms include Scale Invariant Feature Transform (SIFT), Histogram Oriented Gradient (HOG), Local Binary Pattern (LBP). Learning algorithms that are applied on these features are e.g.: Support Vector Machines (SVM), Random Forest (RF), Principal Component Analysis (PCA), Kernel PCA (KPCA), Linear Decrement Analysis (LDA), Fisher Decrement Analysis (FDA) and many more \ref{fig:MLOverview}. New Machine Learning methods are able to automatically learn feature representations which is much less labor intensive and often very surprising as features are learned that don't intuitively make sense [TODO: show some examples] but perform really well, but sometimes things are learned that correlate with the provided labels but have no causality [TODO: some counter examples].

\begin{figure}[H]
  \centering
  \caption{Partial taxonomy of Artificial Inteligence \cite{alom2018history}}
  \includegraphics[scale=0.4]{chapter3/MLOverview}
  \label{fig:MLOverview}
\end{figure}

An Artificial Neural Network (ANN) tries to mimic the behavior of the human brain to a primitive extent. The analogy of the artificial neuron helps to understand the parallels between how biological neurons pass information but it is only an analogy and therefore should be used very cautiously. There are more differences than similarities and the human brain is still in many aspects a mystery to be solved. Nonetheless the abstract notion of a neuron is visually very appealing and useful. Figure \ref{fig:Neurons} show on the left side a biological neuron with all its relevant parts and the mathematical representation of it on the right side. The biological neuron receives its inputs from other neurons through its dendrites (through many different neurotransmitters in the synapses). If a certain threshold is reached in the cell body from all its dendrites, it transmits the information along the axon to other neurons. The axon branches out at the and reaches several different neurons and the process continues. Learning is believed to happen when the sensitivity of the synapses changes and new pathways are formed through branching the axon and connecting to new axons or making the current pathways stronger.

\begin{figure}[H]
  \centering
  \caption{A biological neuron on the left versus the mathematical representation of neuron how it is used in neural networks on the right. \cite{cs231neuralnetworks}}
  \includegraphics[scale=0.35]{chapter3/Neurons}
  \label{fig:Neurons}
\end{figure}

This is indeed similar to how an artificial neuron processes information and passes it forward. An artificial neuron receives inputs (e.g. $x_0$) from other neurons and applies a weight matrix (e.g. $w_0$) and a bias (e.g. $b$) on it. The weights and biases can be compared to the strength of the synapses in the biological neuron and these can be changed through learning. Actually that's what happens during the backpropagation explained later on. The weights are adjusted in order to classify the input more accurately. In the cell body of the mathematical representation performs a dot product between the inputs and the weights adds the bias ($w_i x_i + b$) and then applies some kind of activation function. This activation function needs to be non-linear, because otherwise all the matrix computations of different neurons could be collapsed into one linear computation and no learning would take place. Just a linear regression would be achieved.
Traditionally a sigmoid function has been used for the activation function since it takes any real number as inputs and outputs numbers between 0 and 1. That is a very intuitive non-linearity function to work with but has some disadvantages when backpropagating the loss function and adjusting the parameters in the weight matrices.  During backpropagation the loss is computed according to some loss function and then back propagated through all layers and units. During that process the partial derivatives of the loss function are computed with respect to the input variables (e.g. $x_i^j$) of that specific unit. In the used notation the superscript denotes the layer whereas the subscript denotes the unit. These derivatives are then multiplied by a learning rate alpha and added to the current weights. This process of changing the weights towards a optimal loss is called learning and happens through many iterations. In Figure \ref{fig:SigmoidTanh} the two activation functions, the sigmoid and the tanh, are shown. Since the partial derivatives are crucial in learning - the stepper the derivation the bigger the change in weights - the disadvantages of both non-linearties are clearly visible towards the borders. If the function moves mostly horizontally, the first derivation will be near 0 thus learning happens very slow. The tanh has a much steeper function around the center (leading to a bigger first derivative) but still has the same problem towards the borders.

\begin{figure}[H]
  \centering
  \caption{The sigmoid non-linearity takes in any real number and outputs a number in the range [0,1], wheras the tanh non-linearity takes the same input but outputs a number in the range of [-1,1]. \cite{cs231neuralnetworks}}
  \includegraphics[scale=0.35]{chapter3/SigmoidTanh}
  \label{fig:SigmoidTanh}
\end{figure}

For the above mentioned reasons, the sigmoid non-linearity is not used anymore except in the last layer and only if the task is a binary classification. For that special case the sigmoid is still a valid function. As an alternative the Rectified Linear Unit (ReLU) is used. It is very simple, fast to compute and easy to back-propagate since the derivatives are 0 or a fixed value. In order to omit the deactivation of units, which happens if the input values are smaller than 0, a leaky ReLU can be used. Both non-linearities can be seen in Figure \ref{fig:ReLU}.

\begin{figure}[H]
  \centering
  \caption{Rectified Liner Unit on the right side. Deactivating units if their input values are negative. Leaky ReLU that never deactivates units. \cite{reinventingNN}}
  \includegraphics[scale=0.35]{chapter3/ReLU}
  \label{fig:ReLU}
\end{figure}

The components of an Artificial Neural Network (ANN) are the units (neurons) organized in an acyclic graph. All units that are reached from the input layer by the same amount of hops are organized into layers. These units are not connected to each other but to next layer as shown in Figure \ref{fig:MLP}. As the name suggests cycles are not allowed since the forward and backward pass would not be well defined anymore. Another big difference to the human brain where acyclic connections are possible and very common [TODO: find relevant reference]. Subsequent layers within the ANN are most commonly fully connected with each other, meaning every unit connects to every unit in the next layer. These networks are also often called Multi-Layer Perceptrons (MLP). One of the problems with MLP's is that the fully-connected nature of this architecture increases the number of parameters exponentially. E.g. an RGB image of 256 by 256 pixels as an input size of 196'608 (256x256x3). If this is multiplied with the next layer having 1000 hidden units the result is already 196'608'000 meaning there are roughly 197 million parameters to be held in memory and updated with every iteration just for the first layer. Every unit in this layer would have 196'608 weights which is wasteful since so many parameters with rather simple images lead to severe overfitting. Another big disadvantage for image recognition is the fact that there is no spatial encoding in the network. Is is very nicely observed with the MNIST dataset where images (28x28 pixels) of handwritten digits are provided \cite{MNISTdatabase}. The MLP learns that the image contains a certain number only by learning which pixels need to be black in order to match the previously seen labels. That leads to a near perfect recognition of the centered digit 2 but if the digit is moved to the border (different distribution of black pixels) the MLP is not able to recognize the digit anymore.

\begin{figure}[H]
  \centering
  \caption{. \cite{cs231nconvolution}}
  \includegraphics[scale=0.36]{chapter3/MLP}
  \label{fig:MLP}
\end{figure}

\section{CNN}

Convolutional Neural Networks (CNN) are a subset of Neural Networks so they still share most of the components like an input layer, hidden layers with its units, fully connected layers at the end of the architecture, a loss function, forward and backward propagation. The big difference is that CNN's explicitly expect images as input whereas a Neural Network expects nothing in particular, just a single vector. Knowing that the input is an image, the architecture can be changed accordingly, leading to spatial awareness through convolutions and vastly reduced parameters. Spatial awareness means that additionally to knowing the value of one pixel (in the first layer) a convolution encodes the values of neighboring pixels, thus giving some representation of an area of nearby pixels. Figure \ref{fig:Convolution} shows a simple convolution step where a filter of 5x5x3 is layed over the image. The filters depth (3rd dimension) is always equal to the input volumes dimension. Since the image is RGB encoded the dept of the image is 3, so are the filter in the first layer. The filter, sometimes also called kernel, is then convolved across the whole image and computes a dot product at every position. This gives an activation map, seen in blue on the right side in Figure \ref{fig:Convolution}. Each filter that is convolved across the image produces a different activation map that usually is slightly smaller than the original input but becomes deeper with every additional layer, since more filters are added. The goal of the learning process is to learn filter, that represent specific characteristics of the image and help in correct classification (reducing the loss). As mentioned in the beginning of the chapter, a huge advantage of modern machine learning methods is that the feature maps are learnt automatically by the model. The best filters will get found without the need of manually crafting specific HOG features. That saves time and leads to much more generalizable models.

\begin{figure}[H]
  \centering
  \caption{. \cite{cs231nconvolution}}
  \includegraphics[scale=0.35]{chapter3/Convolution}
  \label{fig:Convolution}
\end{figure}

Instead of connecting all the neurons of one layer to all the neurons of the subsequent layer leading to an exponential growth of parameters, now each neuron is only connected to a local region of the input. The 5x5x3 filter shown in Figure \ref{fig:Convolution} has receptive filed of 5x5. The depth of the filter must always be equal to the depth of the input volume (input image in the first layer).

In order to build a CNN three main components are used. The convolution layer which is described above often followed by a Pooling Layer that reduces the width and height, while keeping the depth as is. In the end the third component is a Fully Connected Layer. Often the Fully-Connected Layer also consists of a softmax function that gives the probability of the image being one of each classes. Between the Convolution Layer and the Pooling Layer the activation function (e.g. ReLU) applies elementwise. The activation function does not change the dimensions but simply squashes the values into the range of [0,x] in case of a ReLU. A typical Convolutional Network would have the following architecture [INPUT - x * (CONV - RELU - POOL) - FC]. The higher the variable x the deeper the CNN.

The Pooling Layer's goal is to reduce the width and height of the input volume without reducing its depth. That leads to less parameters in the network and less overfitting. The architecture needs to focus on the relevant parts of the image instead of trying to optimize too many parameters and thus overfit to already seen images. Pooling Layers mostly apply the MAX function and are of size 2x2. The MAX function is computationally very cheap and the size of 2x2 let's the volume half in width and height. The highest value in the 2x2 mask is kept. Figure \ref{fig:MaxPooling} shows the MAX-Pooling process.

\begin{figure}[H]
  \centering
  \caption{. \cite{cs231convnetworks}}
  \includegraphics[scale=0.35]{chapter3/MaxPooling}
  \label{fig:MaxPooling}
\end{figure}


\section{ImageNet Large Scale Visual Recognition Challenge (ILSVRC)}

The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has become the standard benchmark for object recognition and is held anually since 2010 \cite{imagenet}. It has borrowed many concepts from the PASCAL VOC 2010 dataset but increased the amount of data drastically, from 19'737 images in PASCAL VOC 2010 with 20 object classes to 1'461'406 images and 1000 object classes in ILSVRC 2010 \cite{everingham2010pascal, russakovsky2015imagenet}. ILSVRC consists of two components: the dataset that is publicly available and consists of over 1.2 million annotated, high-resolution images (train set), and the annual competition with its workshops. The test set, ranging from 100'000 to 150'000 images, is also released but without the labels. Participants are able to train their models on the datasets and annotate the images in the test set. The annotated test set is then send to the evaluation server where the overall score is computed \cite{russakovsky2015imagenet}. With the ImageNet Challenge there are two main competitions namely object detection for 200 fully labeled categories and object localization for 1000 categories (binary label for presence or absence of an object). For the object localization there are two main metrics, the top-1 error rate and the top-5 error rate. The top-1 error rate shows the percentage for which the correct class was failed to be correctly predicted (only the first top label is observed). The top-5 error rate shows the percentag of test examples for which the correct label was not in the top 5 predicted classes. Since ILSVRC 2015 new competition tasks are added like object detection from video or scene classificiation, but they are less relevant for this thesis \cite{imagenet2015}. Before ILSVRC many datasets were available for training models like Caltech 101 \cite{fei2007learning} with 101 object classes and Caltech 256 \cite{griffin2007caltech} with 256 object classes. Both were tiny at best when compared to the ImageNet dataset. TinyImages \cite{torralba200880} is huge with its 80 million images but the images are only 32 by 32 pixels which makes them low-resolution and not manually verified thus leading to many incorrect labels.

In 2010 and 2011 the winners used traditional machine learning approaches like HOG, LBP and FV for dense grid descriptors and SVM for feature application with other more specialized methods. In 2012 AlexNet, a 5-layer CNN, beat all competitor by a large margin. It was the beginning of a new chapter in machine learning namely the chapter of Convolutional Neural Networks and Deep Learning in the field of computer vision. Since then, only CNN's are winning the ILSVRC with ResNet surpassing human level accuracy in 2015 \cite{he2016deep}. Human level accuracy was obtained when Andrej Karpathy took it on and labeled 1'500 test images after having trained on 500 validation images. It took him a few days and he reached an astonishing 5.1\% top-5 error rate while other scientists giving it a try reached around 12\% error rate \cite{humanlevel2014}.

It is also worth noting that ImageNet is the most widely used dataset for transfer learning. Pre-trained weights for most of the current state-of-the-art architectures are widely available on the internet and can be downloaded. All evaluation in Chapter 5 related to transfer learning is based on weights pre-trained on ImageNet.

\section{AlexNet}

In 2012, a paper by Alex Krizhevsky started the new era of Convolutional Neural Networks, where CNN's were not only practibly applicable to current visual problems like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), but beat the best results by a big margin \cite{krizhevsky2012imagenet, imagenet}. The architecture by Alex Krizhevsky, a 5-layer CNN, achieved 15.3\% top-5 error rate, and was named AlexNet after its creator. The next best result achieved a top-5 error rate of 26.2\% with versions of Fisher Vectors (FV) and Graphical Gaussian Vectors (GGV) after going through a feature extraction process with SIFT, CSIFT, LBP and GIST. Since then the winners of all subsequent ILSVRC challenges apply all successfully a deep learning approach. \\

AlexNet uses eight learned layers of which five are convolutional, and three are fully-connected layers. After the convolutions, ReLU's are used for the activation function since they train several times faster than equivalent architectures trained with tanh or sigmoid functions. The last fully-connected layer is fed to a 1000-way softmax function that produces a distribution over all the 1000 classes giving the probability of a certain class to be contained in the image. For the input layer the images are first resized to 256 by 256 pixels and then crops of size 227 by 227 by 3 are used as inputs into the model. 96 kernels of size 11 by 11 by 3 with a stride of 4 pixels are then applied on the input image leading to a output volume of 55 by 55 by 96. In the original paper the filters were split across two GPU's but since the GPU's have increased their capacity and performance since 2012 hugely, this splitting is not applied on the current model. The formula for calculating the new output volumes is the following: \\

\[ O = {\frac{(W - F + 2P)}{S}} + 1 \] \\

with O standing for the output dimension (length of the width or height), W standing for the input length, F for the filter size, P for padding and S for stride. Maxpooling is done after the first, second and fifth convolutions. Instead of going with the regular 2x2 pooling that halfes the width and length dimensions of the input volume, AlexNet applies a 3x3 pooling with stride 2. They argue that it helps in reducing overfitting and leads to slightly better accuracies. The architecture can be seen in figure \ref{fig:AlexNet}. \\

\begin{figure}[H]
  \centering
  \caption{The original architecture as proposed by Alex Krizhevsky with the distribution of the model and filter over two GPU's. \cite{krizhevsky2012imagenet}}
  \includegraphics[scale=0.35]{chapter3/AlexNet}
  \label{fig:AlexNet}
\end{figure}

The horizontal seperation of the architecture comes from the need to distribute the model and the filters over two GPU's. Unfortunately the image was already cropped that way in the original paper of Alex Krizhevsky. \\

Although the architecture itself is very small compared to the later architectures, it still has over 60 million parameters to learn. These parameters are mostly located in the first and second fully connected layer. Learning so many parameters for classifying only 1000 objects leads to overfitting to the data trained on. For the ILSVRC competition two methods have been used. Heavy data augmentation using label-preserving transformations (increase images by factor 2048) and dropout in which certain hidden units are dropped for the current forward pass with some probability (often 0.5). \\

The winner of the ILSVRC 2013 was ZFNet which basically is the AlexNet architecture with very small alterations \cite{zeiler2014visualizing}. Although the alterations are small they were very guided in their developement through visualization techniques. Zeiler and Fergus visualized the layers and filters and deduced clear and specific improvements on AlexNet.

Karen Simonyan and Andew Zisserman

\section{VGG}

The winner of the ILSVRC 2014 were Karen Simonyan and Andrew Zisserman with their architecture named VGG \cite{simonyan2014very}. Instead of further trying to fine-tune AlexNet they explicitly wanted to explore how depth of an architecture plays out on accuracy and generalizability. For that reason they used only 3 x 3 convolutions with padding of 1 pixel and stride of 1 pixel as well followed by a ReLU activiation function. Therefore the input volume from layer to layer stays the same while the number of filter increase after each maxpooling layer. If the spatial resolution needs to be reduced, maxpooling is applied with a 2 by 2 windows and stride 2. This allowed Simonyan et al. to push the depth of the network to 16-19 layers. In the end there are again 3 fully-connected layers as in AlexNet. The first two have 4096 hidden units and the third performs a softmax on the 1000 classes of ImageNet dataset. Figure shows the original VGG16 architecture.

\begin{figure}[H]
  \centering
  \caption{VGG16 architecture with 13 convolutional layers and three fully-connected layers at the end. \cite{ferguson2017automatic}}
  \includegraphics[scale=0.35]{chapter3/VGG16}
  \label{fig:vgg16}
\end{figure}

One of the bigger issues with VGG architectures are their huge size in parameters, which ranges from 133 million parameters for the smaller VGG11 architecture to 144 million parameters for the bigger VGG19. This is problem, since it can be difficult to handle so many parameters and training takes much longer. The last three fully-connected layers contribute the large part of the parameters with the convolutional layers playing a minor role. Because VGG is very uniform with it's 1 type of convolution and easy to understand, it enjoyed large acceptance with the community. Figure \ref{fig:parameters} shows how different architectures perform relative to their size in parameters.

\begin{figure}[H]
  \centering
  \caption{Performance of different architectures relative to their number of operations needed for one forward pass. The number of operations is proportional to the number of parameters used by the architecture. \cite{canziani2016analysis}}
  \includegraphics[scale=0.35]{chapter3/parameters}
  \label{fig:parameters}
\end{figure}

\section{Inception}

Explain Inception and why it's so amazing

\section{ResNet}

The winner of the ILSVRC 2015 were Kaiming He et al. with their very deep architectures applying residual connection connections \cite{he2016deep, he2016identity}. Their architecture was named ResNet and was specifically created with depth in mind. Zeiler et al. have shown how deep networks are naturally able to integrate low/mid/high-level features and that depth seems to enrich these features leading to better performance. Leading results in the ImageNet challenge like the aformentioned VGG \cite{simonyan2014very} and LeNet \cite{szegedy2015going} all exploit deeper architectures. There are a few computational obstacle of going simply deeper with the current models except computational power. One issue arises from the vanishing/exploding gradient which happens during backpropagation \cite{glorot2010understanding}. The weights get updated during learning by multiplying the weights with the gradient from the partial derivatives of the error function. This leads to the chain rule being apllied as many times as layers are present in the network. If the gradient is some small number below 0 it gets multiplied many times over leading to a vanishing value that makes is very difficult to train the layers in the beginning of the network. The same holds true if the gradients are big or above 1. Then they explode by getting bigger and bigger with every layer. But even if the vanishing/exploding gradient can be mitigated with better weight initialization and intermediate normalization layers \cite{ioffe2015batch}, the degradation problem prohibits learning of deeper networks, where the accuracy gets saturated and starts degrading with more layers \cite{he2016deep}. This observed degradation is not caused by overfitting to the training data \cite{he2015convolutional, srivastava2015highway}.\\

ResNet architectures address this degradation problem by introducing residual mappings. These residual mappings are easier to optimize than the unreferenced mapping. If for example it would be optimal to apply the identity mapping, pushing the residual mapping to zero is much easier for the network than fit an identity mapping by a stack of nonlinear layers. Figure \ref{fig:ResidualBlock} shows such a residual block.

\begin{figure}[H]
  \centering
  \caption{A building block for residual learning. \cite{he2016deep}}
  \includegraphics[scale=0.35]{chapter3/ResidualBlock}
  \label{fig:ResidualBlock}
\end{figure}

As seen in Figure \ref{fig:ResNet} blabla blabla

\begin{figure}[H]
  \centering
  \caption{Comparison of the VGG-19 architecture with a 34-layer plain model and its residual counterpart. Notice that only the VGG architecture has 3 fully-connected layers. By omitting these 3 fully-connected layers in the ResNet architecture the overall complexity is drastically reduced. \cite{he2016deep}}
  \includegraphics[scale=0.35]{chapter3/ResNet}
  \label{fig:ResNet}
\end{figure}

\section{Densenet}

Explain Resnet and why it's so amazing

