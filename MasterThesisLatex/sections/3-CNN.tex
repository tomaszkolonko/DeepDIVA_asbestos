\chapter{Deep Learning}

Deep Learning (DL) is a subfield of Neural Networks (NN) which in turn is a subfield of Machine Learning (ML) which again is a subfield of Artificial Intelligence (AI). Figure \ref{fig:MLOverview} shows a Neural Network relevant taxonomy of AI. There are many advantages of ML over more traditional statistical methods as often found in statistical learning. Some of them are that ML does not require any hypothesis and deep understanding of the underlying data. While statistical learning is mostly about inference (deducing properties of an underlying probability distribution, sampled from a larger population), ML is mostly about predictions in supervised, unsupervised and semi-supervised learning. It also does not operate on assumptions like normality, multicollinearity and homoscedasticity etc.. Statistical Learning operates on much smaller datasets with only a few attributes and therefore is less fit for problems with millions of possible and possibly unknown attributes and data samples. Machine Learning excels at identifying patterns from large datsets through iterations and being able to predict or classify previously unseen data.
Traditional Machine Learning approaches were based on very specific feature extractions that needed to be found/created manually and could take months to fine tune. The biggest downside was that the features often were not generalizable but were very domain specific. Even looking at the same object from different angles or from different photographs often needed additional fine tuning of existing features or creating new ones. Some of the feature extraction algorithms include Scale Invariant Feature Transform (SIFT), Histogram Oriented Gradient (HOG), Local Binary Pattern (LBP). Learning algorithms that are applied on these features are e.g.: Support Vector Machines (SVM), Random Forest (RF), Principal Component Analysis (PCA), Kernel PCA (KPCA), Linear Decrement Analysis (LDA), Fisher Decrement Analysis (FDA) and many more \ref{fig:MLOverview}. New Machine Learning methods are able to automatically learn feature representations which is much less labor intensive and often very surprising as features are learned that don't intuitively make sense [TODO: show some examples] but perform really well, but sometimes things are learned that correlate with the provided labels but have no causality [TODO: some counter examples].

\begin{figure}[H]
  \centering
  \caption{Partial taxonomy of Artificial Inteligence \cite{alom2018history}}
  \includegraphics[scale=0.4]{chapter3/MLOverview}
  \label{fig:MLOverview}
\end{figure}

An Artificial Neural Network (ANN) tries to mimic the behavior of the human brain to a primitive extent. The analogy of the artificial neuron helps to understand the parallels between how biological neurons pass information but it is only an analogy and therefore should be used very cautiously. There are more differences than similarities and the human brain is still in many aspects a mystery to be solved. Nonetheless the abstract notion of a neuron is visually very appealing and useful. Figure \ref{fig:Neurons} show on the left side a biological neuron with all its relevant parts and the mathematical representation of it on the right side. The biological neuron receives its inputs from other neurons through its dendrites (through many different neurotransmitters in the synapses). If a certain threshold is reached in the cell body from all its dendrites, it transmits the information along the axon to other neurons. The axon branches out at the and reaches several different neurons and the process continues. Learning is believed to happen when the sensitivity of the synapses changes and new pathways are formed through branching the axon and connecting to new axons or making the current pathways stronger.

\begin{figure}[H]
  \centering
  \caption{A biological neuron on the left versus the mathematical representation of neuron how it is used in neural networks on the right. \cite{cs231neuralnetworks}}
  \includegraphics[scale=0.35]{chapter3/Neurons}
  \label{fig:Neurons}
\end{figure}

This is indeed similar to how an artificial neuron processes information and passes it forward. An artificial neuron receives inputs (e.g. $x_0$) from other neurons and applies a weight matrix (e.g. $w_0$) and a bias (e.g. $b$) on it. The weights and biases can be compared to the strength of the synapses in the biological neuron and these can be changed through learning. Actually that's what happens during the backpropagation explained later on. The weights are adjusted in order to classify the input more accurately. In the cell body of the mathematical representation performs a dot product between the inputs and the weights adds the bias ($w_i x_i + b$) and then applies some kind of activation function. This activation function needs to be non-linear, because otherwise all the matrix computations of different neurons could be collapsed into one linear computation and no learning would take place. Just a linear regression would be achieved.
Traditionally a sigmoid function has been used for the activation function since it takes any real number as inputs and outputs numbers between 0 and 1. That is a very intuitive non-linearity function to work with but has some disadvantages when backpropagating the loss function and adjusting the parameters in the weight matrices.  During backpropagation the loss is computed according to some loss function and then back propagated through all layers and units. During that process the partial derivatives of the loss function are computed with respect to the input variables (e.g. $x_i^j$) of that specific unit. In the used notation the superscript denotes the layer whereas the subscript denotes the unit. These derivatives are then multiplied by a learning rate alpha and added to the current weights. This process of changing the weights towards a optimal loss is called learning and happens through many iterations. In Figure \ref{fig:SigmoidTanh} the two activation functions, the sigmoid and the tanh, are shown. Since the partial derivatives are crucial in learning - the stepper the derivation the bigger the change in weights - the disadvantages of both non-linearties are clearly visible towards the borders. If the function moves mostly horizontally, the first derivation will be near 0 thus learning happens very slow. The tanh has a much steeper function around the center (leading to a bigger first derivative) but still has the same problem towards the borders.

\begin{figure}[H]
  \centering
  \caption{The sigmoid non-linearity takes in any real number and outputs a number in the range [0,1], wheras the tanh non-linearity takes the same input but outputs a number in the range of [-1,1]. \cite{cs231neuralnetworks}}
  \includegraphics[scale=0.35]{chapter3/SigmoidTanh}
  \label{fig:SigmoidTanh}
\end{figure}

For the above mentioned reasons, the sigmoid non-linearity is not used anymore except in the last layer and only if the task is a binary classification. For that special case the sigmoid is still a valid function. As an alternative the Rectified Linear Unit (ReLU) is used. It is very simple, fast to compute and easy to back-propagate since the derivatives are 0 or a fixed value. In order to omit the deactivation of units, which happens if the input values are smaller than 0, a leaky ReLU can be used. Both non-linearities can be seen in Figure \ref{fig:ReLU}.

\begin{figure}[H]
  \centering
  \caption{Rectified Liner Unit on the right side. Deactivating units if their input values are negative. Leaky ReLU that never deactivates units. \cite{reinventingNN}}
  \includegraphics[scale=0.35]{chapter3/ReLU}
  \label{fig:ReLU}
\end{figure}

The components of an Artificial Neural Network (ANN) are the units (neurons) organized in an acyclic graph. All units that are reached from the input layer by the same amount of hops are organized into layers. These units are not connected to each other but to next layer as shown in Figure \ref{fig:MLP}. As the name suggests cycles are not allowed since the forward and backward pass would not be well defined anymore. Another big difference to the human brain where acyclic connections are possible and very common [TODO: find relevant reference]. Subsequent layers within the ANN are most commonly fully connected with each other, meaning every unit connects to every unit in the next layer. These networks are also often called Multi-Layer Perceptrons (MLP). One of the problems with MLP's is that the fully-connected nature of this architecture increases the number of parameters exponentially. E.g. an RGB image of 256 by 256 pixels as an input size of 196'608 (256x256x3). If this is multiplied with the next layer having 1000 hidden units the result is already 196'608'000 meaning there are roughly 197 million parameters to be held in memory and updated with every iteration just for the first layer. Every unit in this layer would have 196'608 weights which is wasteful since so many parameters with rather simple images lead to severe overfitting. Another big disadvantage for image recognition is the fact that there is no spatial encoding in the network. Is is very nicely observed with the MNIST dataset where images (28x28 pixels) of handwritten digits are provided \cite{MNISTdatabase}. The MLP learns that the image contains a certain number only by learning which pixels need to be black in order to match the previously seen labels. That leads to a near perfect recognition of the centered digit 2 but if the digit is moved to the border (different distribution of black pixels) the MLP is not able to recognize the digit anymore.

\begin{figure}[H]
  \centering
  \caption{. \cite{cs231nconvolution}}
  \includegraphics[scale=0.36]{chapter3/MLP}
  \label{fig:MLP}
\end{figure}

\section{CNN}

Convolutional Neural Networks (CNN) are a subset of Neural Networks so they still share most of the components like an input layer, hidden layers with its units, fully connected layers at the end of the architecture, a loss function, forward and backward propagation. The big difference is that CNN's explicitly expect images as input whereas a Neural Network expects nothing in particular, just a single vector. Knowing that the input is an image, the architecture can be changed accordingly, leading to spatial awareness through convolutions and vastly reduced parameters. Spatial awareness means that additionally to knowing the value of one pixel (in the first layer) a convolution encodes the values of neighboring pixels, thus giving some representation of an area of nearby pixels. Figure \ref{fig:Convolution} shows a simple convolution step where a filter of 5x5x3 is layed over the image. The filters depth (3rd dimension) is always equal to the input volumes dimension. Since the image is RGB encoded the dept of the image is 3, so are the filter in the first layer. The filter, sometimes also called kernel, is then convolved across the whole image and computes a dot product at every position. This gives an activation map, seen in blue on the right side in Figure \ref{fig:Convolution}. Each filter that is convolved across the image produces a different activation map that usually is slightly smaller than the original input but becomes deeper with every additional layer, since more filters are added. The goal of the learning process is to learn filter, that represent specific characteristics of the image and help in correct classification (reducing the loss). As mentioned in the beginning of the chapter, a huge advantage of modern machine learning methods is that the feature maps are learnt automatically by the model. The best filters will get found without the need of manually crafting specific HOG features. That saves time and leads to much more generalizable models.

\begin{figure}[H]
  \centering
  \caption{. \cite{cs231nconvolution}}
  \includegraphics[scale=0.35]{chapter3/Convolution}
  \label{fig:Convolution}
\end{figure}

Instead of connecting all the neurons of one layer to all the neurons of the subsequent layer leading to an exponential growth of parameters, now each neuron is only connected to a local region of the input. The 5x5x3 filter shown in Figure \ref{fig:Convolution} has receptive filed of 5x5. The depth of the filter must always be equal to the depth of the input volume (input image in the first layer).

In order to build a CNN three main components are used. The convolution layer which is described above often followed by a Pooling Layer that reduces the width and height, while keeping the depth as is. In the end the third component is a Fully Connected Layer. Often the Fully-Connected Layer also consists of a softmax function that gives the probability of the image being one of each classes. Between the Convolution Layer and the Pooling Layer the activation function (e.g. ReLU) applies elementwise. The activation function does not change the dimensions but simply squashes the values into the range of [0,x] in case of a ReLU. A typical Convolutional Network would have the following architecture [INPUT - x * (CONV - RELU - POOL) - FC]. The higher the variable x the deeper the CNN.

The Pooling Layer's goal is to reduce the width and height of the input volume without reducing its depth. That leads to less parameters in the network and less overfitting. The architecture needs to focus on the relevant parts of the image instead of trying to optimize too many parameters and thus overfit to already seen images. Pooling Layers mostly apply the MAX function and are of size 2x2. The MAX function is computationally very cheap and the size of 2x2 let's the volume half in width and height. The highest value in the 2x2 mask is kept. Figure \ref{fig:MaxPooling} shows the MAX-Pooling process.

\begin{figure}[H]
  \centering
  \caption{. \cite{cs231convnetworks}}
  \includegraphics[scale=0.35]{chapter3/MaxPooling}
  \label{fig:MaxPooling}
\end{figure}

\section{AlexNet}



\section{Inception}

Explain Inception and why it's so amazing

\section{ResNet}

Explain Resnet and why it's so amazing
